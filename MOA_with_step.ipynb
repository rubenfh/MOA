{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MOA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VUYhjXsZZwDz",
        "sSMtuIw6asM0",
        "IDCwpYGtbWXH",
        "pn0PWHC8boIW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenfh/MOA/blob/dev%2Falex/MOA_with_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNwOy7p6ZqFU"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "In this notebook, we will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeByYlPabr7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "9ca1bbfc-3b62-40e6-d73a-720c23a00012"
      },
      "source": [
        "import os \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "os.chdir(\"/content/drive/My drive/ING5/Deep Learning/MoA\")\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e875211e93e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My drive/ING5/Deep Learning/MoA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My drive/ING5/Deep Learning/MoA'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw3Xf8P1aD5A"
      },
      "source": [
        "# **Step 1:** Reading data using TF.DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCa_umGDZJzb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "TRAIN_DATA_URL = \"https://drive.google.com/file/d/1cWd2o55ed-pf-Ia9umqJGTOlCFydwgyf/view?usp=sharing\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haPjCN6aKHKB"
      },
      "source": [
        "BATCH_SIZE=2048\n",
        "dl = tf.data.experimental.make_csv_dataset(file_pattern = \"train_features.csv\",batch_size=BATCH_SIZE, num_epochs=1,\n",
        "    num_parallel_reads=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2qG5oI1KTVG"
      },
      "source": [
        "%%time\n",
        "for i,batch in enumerate(dl.take(20)):\n",
        "  print('.',end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMq1n2gqKWCg"
      },
      "source": [
        "num_elements = 0\n",
        "for element in dl:\n",
        "    print(element)\n",
        "    num_elements += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1SAzdpkn8G"
      },
      "source": [
        "### Split the training dataset into training and validation to be able to perform the hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "129s2-c7b9Dq"
      },
      "source": [
        "DATASET_SIZE = num_elements\n",
        "print(DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mvJ2s7VKsND"
      },
      "source": [
        "train_size = int(0.7 * DATASET_SIZE)\n",
        "val_size = int(0.15 * DATASET_SIZE)\n",
        "test_size = int(0.15 * DATASET_SIZE)\n",
        "\n",
        "train_dataset = dl.take(train_size)\n",
        "val_dataset = dl.skip(train_size)\n",
        "test_dataset = dl.skip(val_size)\n",
        "val_dataset = dl.take(val_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJvAuSL9ZWkx"
      },
      "source": [
        "### Shuffing, Batching, Mapping data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX0Pkw57KyQM"
      },
      "source": [
        "for batch in dl.take(1):\n",
        "      for key, value in batch.items():\n",
        "        print(f\"{key:20s}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Yd1xTokVYG"
      },
      "source": [
        "shuffle_train_dataset = train_dataset.cache().shuffle(1000)\n",
        "batched_train_dataset = shuffle_train_dataset.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYhjXsZZwDz"
      },
      "source": [
        "#**Step 2**: Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3FB8bpsaKtu"
      },
      "source": [
        "### We use tensorflow\tfeature_columns or keras preprocessing\tlayers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrl4IY67aDHX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuF31KlVaRS6"
      },
      "source": [
        "### Now we differentiate between numerical features and categorical features and apply the corresponding adequate feature engineering strategies to the right format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuyA2MIKaZJV"
      },
      "source": [
        "Normalization of numerical columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lddjcFNMaY01"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxYJQxtLan_l"
      },
      "source": [
        "One hot encoding of categorical columns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ-fVkcfar-J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMtuIw6asM0"
      },
      "source": [
        "#**Step 3:** Baseline Modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39BRjpLqa4po"
      },
      "source": [
        "Using either the Keras sequential API or the Keras function API, we should have at least one hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4HBACMaayDU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG9_EOpgbB5g"
      },
      "source": [
        "L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy5AJNc9bFBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3CSNUyJbFVd"
      },
      "source": [
        "Dropout regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1_XzuVbHUO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5W8IetMbGuG"
      },
      "source": [
        "Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn0RNmICbICM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvPMwftbLlF"
      },
      "source": [
        "Weights Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzz79AQBbNm6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDCwpYGtbWXH"
      },
      "source": [
        "#**Step 4:** Model Variance & Bias Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP5Zf6lsbZII"
      },
      "source": [
        "Plot the training and validation loss of your model as a function of the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KlsqXPubXNx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWj4Opgbbbiz"
      },
      "source": [
        "Plot the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBt2LGhebkVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08PCoGm5bktt"
      },
      "source": [
        "Diagnose Variance & Bias and propose a way to solve high variance or high bias if itâ€™s being detected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP04Is88bnyk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0PWHC8boIW"
      },
      "source": [
        "#**Steps 5:** Hyperparameter tuning with Keras Tuner "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deqDKzrLbyTX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}