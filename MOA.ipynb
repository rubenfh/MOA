{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MOA",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKPKDgpxmX25jYMIv2kE7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenfh/MOA/blob/dev%2Falex/MOA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TepgBQy2FG07"
      },
      "source": [
        "# ML library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow import feature_column\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data manipulation library\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "#Other library\n",
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M_f_jW9FN_z"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    TRAIN_FEATURES_PATH =  \"drive/MyDrive/lish-moa/train_features.csv\"\n",
        "    TRAIN_LABELS_PATH   =  \"drive/MyDrive/lish-moa/train_targets_scored.csv\"\n",
        "    TEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/test_features.csv\"\n",
        "    BEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/best_features.csv\"\n",
        "else:\n",
        "    TRAIN_FEATURES_PATH =  \"lish-moa/train_features.csv\"\n",
        "    TRAIN_LABELS_PATH   =  \"lish-moa/train_targets_scored.csv\"\n",
        "    TEST_FEATURES_PATH  =  \"lish-moa/test_features.csv\"\n",
        "    BEST_FEATURES_PATH  =  \"lish-moa/best_features.csv\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4DSQ1vFQcI",
        "outputId": "14bdcdc0-ea44-437f-a3fc-f6142a7a6646"
      },
      "source": [
        "features = pd.read_csv(TRAIN_FEATURES_PATH, nrows=10)\n",
        "targets = pd.read_csv(TRAIN_LABELS_PATH, nrows=10)\n",
        "#best_features = pd.read_csv(BEST_FEATURES_PATH)\n",
        "cols_features = features.columns\n",
        "cols_targets = targets.columns\n",
        "\n",
        "num_features = len(cols_features)\n",
        "num_targets = len(cols_targets)\n",
        "print(\"Number of features:\" , num_features)\n",
        "print(\"Number of targets:\" , num_targets)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features: 876\n",
            "Number of targets: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV2rhk9VFS0-"
      },
      "source": [
        "features_types = [str(), str(), str(), str()] + [float()]*(num_features-4)\n",
        "targets_types = [str()] + [float()]*(num_targets-1)\n",
        "\n",
        "features = tf.data.experimental.CsvDataset(TRAIN_FEATURES_PATH,\n",
        "                                           record_defaults=features_types,\n",
        "                                           #select_cols\n",
        "                                           header=True)\n",
        "\n",
        "targets = tf.data.experimental.CsvDataset(TRAIN_LABELS_PATH,\n",
        "                                          record_defaults=targets_types,\n",
        "                                          header=True)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((features, targets))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcQFQfg_FTwC",
        "outputId": "611fec03-849c-40d0-fcc6-101e7797ce72"
      },
      "source": [
        "# split dataset into train and val\n",
        "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "train_size = int(0.7*dataset_size)\n",
        "val_size = int(0.15*dataset_size)\n",
        "test_size = int(0.15*dataset_size)\n",
        "\n",
        "train = dataset.take(train_size)\n",
        "val = dataset.skip(train_size)\n",
        "val = dataset.take(val_size)\n",
        "test = dataset.skip(train_size + val_size)\n",
        "test = dataset.take(test_size)\n",
        "\n",
        "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
        "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
        "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "print(\"Full dataset size:\", dataset_size)\n",
        "print(\"Train dataset size:\", train_size)\n",
        "print(\"Val dataset size:\", val_size)\n",
        "print(\"Test dataset size:\", test_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full dataset size: 23814\n",
            "Train dataset size: 16669\n",
            "Val dataset size: 3572\n",
            "Test dataset size: 3572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0LkSqsSFXm7"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def _preprocess_line(features, targets):\n",
        "    # Pack the result into a dictionary\n",
        "    features = dict(zip(cols_features, features))\n",
        "    features.pop('sig_id')\n",
        "    targets = tf.stack(targets[1:])\n",
        "    return features, targets\n",
        "\n",
        "train = train.map(_preprocess_line)\n",
        "train = train.batch(BATCH_SIZE)\n",
        "\n",
        "val = val.map(_preprocess_line)\n",
        "val = val.batch(BATCH_SIZE)\n",
        "\n",
        "test = test.map(_preprocess_line)\n",
        "test = test.batch(BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VlyC9JXFZas",
        "outputId": "11be79ef-dee3-4acf-8d4b-c7312eb0da9b"
      },
      "source": [
        "for feature_batch, label_batch in train.take(1):\n",
        "    print('First 5 features:', list(feature_batch.keys())[:5])\n",
        "    print('A batch of cp_types:', feature_batch['cp_type'].numpy())\n",
        "    print('A batch of cp_times:', feature_batch['cp_time'].numpy())\n",
        "    print('A batch of targets:', label_batch.numpy() ) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 features: ['cp_type', 'cp_time', 'cp_dose', 'g-0', 'g-1']\n",
            "A batch of cp_types: [b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'ctl_vehicle' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp']\n",
            "A batch of cp_times: [b'24' b'72' b'48' b'48' b'72' b'24' b'24' b'48' b'48' b'48' b'72' b'48'\n",
            " b'48' b'48' b'72' b'48' b'48' b'24' b'72' b'48' b'48' b'48' b'72' b'72'\n",
            " b'72' b'48' b'72' b'48' b'48' b'72' b'72' b'48']\n",
            "A batch of targets: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xfo0egbGOkz"
      },
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = feature_ds.map(index)\n",
        "\n",
        "  # Learn the space of possible indices.\n",
        "  encoder.adapt(feature_ds)\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUM4wEAGVSn"
      },
      "source": [
        "encoded_features = []\n",
        "all_inputs = []\n",
        "for header in cols_features[4:]:\n",
        "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "    encoded_numeric_col= tf.linalg.normalize(numeric_col, ord='euclidean', axis=None, name=None)\n",
        "    all_inputs.append(numeric_col)\n",
        "    encoded_features.append(encoded_numeric_col[0]) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMJDPEc_GnXK",
        "outputId": "9aecc1ed-b836-435b-9d01-1f4d3177cce1"
      },
      "source": [
        "categorical_cols = ['cp_type', 'cp_dose','cp_time']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n",
        "  print(\"Normalization of \", str(header), \" done !\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalization of  cp_type  done !\n",
            "Normalization of  cp_dose  done !\n",
            "Normalization of  cp_time  done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEGz-249GtoL"
      },
      "source": [
        "all_features = []\n",
        "all_features = tf.keras.layers.concatenate(encoded_features)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccm157uaGukc"
      },
      "source": [
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(all_inputs, output)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vsRgMzrXaxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd961e6c-29dc-4d84-cbea-0269d81379cc"
      },
      "source": [
        "model.fit(train,\n",
        "          validation_data=val,\n",
        "          epochs=4)\n",
        "model.save('MOA_model_1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "521/521 [==============================] - 50s 95ms/step - loss: 0.0528 - accuracy: 0.9966 - val_loss: 0.0526 - val_accuracy: 0.9966\n",
            "Epoch 2/4\n",
            "521/521 [==============================] - 49s 93ms/step - loss: 0.0528 - accuracy: 0.9966 - val_loss: 0.0526 - val_accuracy: 0.9966\n",
            "Epoch 3/4\n",
            "521/521 [==============================] - 49s 94ms/step - loss: 0.0528 - accuracy: 0.9966 - val_loss: 0.0526 - val_accuracy: 0.9966\n",
            "Epoch 4/4\n",
            "315/521 [=================>............] - ETA: 15s - loss: 0.0527 - accuracy: 0.9966"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrO-R1ljXi6t",
        "outputId": "99ab9ba7-3643-44ed-a0d6-ec7bf708b24d"
      },
      "source": [
        "loss, accuracy = model.evaluate(test)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112/112 [==============================] - 8s 74ms/step - loss: 0.0526 - accuracy: 0.9966\n",
            "Accuracy 0.9965905547142029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX_CqLUwGv2T",
        "outputId": "aa7942fd-ab0c-4fc2-89eb-cbe828300185"
      },
      "source": [
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
        "#model.save(\"drive/MyDrive/lish-moa\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.223681 to fit\n",
            "tcmalloc: large alloc 1356292096 bytes == 0x555de7198000 @  0x7fe1bef6c001 0x7fe1bb2031fa 0x7fe1bb2032ad 0x7fe1bc38e6df 0x7fe1bc885261 0x7fe1bed02468 0x7fe1bed04d53 0x555de17df092 0x7fe1be6b7bf7 0x555de17df12a\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}