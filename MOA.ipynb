{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MOA",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBC/bpXcZuy9ivZuCD74/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenfh/MOA/blob/dev%2Falex/MOA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TepgBQy2FG07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4731167-e1c8-4111-c15d-a5c47e58ffc0"
      },
      "source": [
        "# ML library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow import feature_column\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import tensorflow_addons as tfa\n",
        "#Tensor Flow doc\n",
        "!pip3 install -q git+https://github.com/tensorflow/docs\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data manipulation library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "#Other library\n",
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import uuid \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9H-jbV4Y7KZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M_f_jW9FN_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322cde1e-0642-4df3-95e1-7c1df013cb7c"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    TRAIN_FEATURES_PATH =  \"drive/MyDrive/lish-moa/train_features.csv\"\n",
        "    TRAIN_LABELS_PATH   =  \"drive/MyDrive/lish-moa/train_targets_scored.csv\"\n",
        "    TEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/test_features.csv\"\n",
        "    BEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/best_features.csv\"\n",
        "else:\n",
        "    TRAIN_FEATURES_PATH =  \"lish-moa/train_features.csv\"\n",
        "    TRAIN_LABELS_PATH   =  \"lish-moa/train_targets_scored.csv\"\n",
        "    TEST_FEATURES_PATH  =  \"lish-moa/test_features.csv\"\n",
        "    BEST_FEATURES_PATH  =  \"lish-moa/best_features.csv\"\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4DSQ1vFQcI",
        "outputId": "19897d19-618b-4da5-b809-66551e8285ce"
      },
      "source": [
        "features = pd.read_csv(TRAIN_FEATURES_PATH, nrows=10)\n",
        "targets = pd.read_csv(TRAIN_LABELS_PATH, nrows=10)\n",
        "#best_features = pd.read_csv(BEST_FEATURES_PATH)\n",
        "cols_features = features.columns\n",
        "cols_targets = targets.columns\n",
        "\n",
        "num_features = len(cols_features) \n",
        "num_targets = len(cols_targets)\n",
        "print(\"Number of features:\" , num_features)\n",
        "print(\"Number of targets:\" , num_targets)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features: 876\n",
            "Number of targets: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV2rhk9VFS0-"
      },
      "source": [
        "features_types = [str(), str(), str(), str()] + [float()]*(num_features-4)\n",
        "targets_types = [str()] + [float()]*(num_targets-1)\n",
        "\n",
        "features = tf.data.experimental.CsvDataset(TRAIN_FEATURES_PATH,\n",
        "                                           record_defaults=features_types,\n",
        "                                           #select_cols\n",
        "                                           header=True)\n",
        "\n",
        "targets = tf.data.experimental.CsvDataset(TRAIN_LABELS_PATH,\n",
        "                                          record_defaults=targets_types,\n",
        "                                          header=True)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((features, targets))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcQFQfg_FTwC",
        "outputId": "be54b6f0-ba60-4528-c726-06b88fc2d30d"
      },
      "source": [
        "# split dataset into train and val\n",
        "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "train_size = int(0.7*dataset_size)\n",
        "val_size = int(0.15*dataset_size)\n",
        "test_size = int(0.15*dataset_size)\n",
        "\n",
        "train = dataset.take(train_size)\n",
        "val = dataset.skip(train_size)\n",
        "val = dataset.take(val_size)\n",
        "test = dataset.skip(train_size + val_size)\n",
        "test = dataset.take(test_size)\n",
        "\n",
        "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
        "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
        "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "print(\"Full dataset size:\", dataset_size)\n",
        "print(\"Train dataset size:\", train_size)\n",
        "print(\"Val dataset size:\", val_size)\n",
        "print(\"Test dataset size:\", test_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full dataset size: 23814\n",
            "Train dataset size: 16669\n",
            "Val dataset size: 3572\n",
            "Test dataset size: 3572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0LkSqsSFXm7"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def _preprocess_line(features, targets):\n",
        "    # Pack the result into a dictionary\n",
        "    features = dict(zip(cols_features, features))\n",
        "    features.pop('sig_id')\n",
        "    targets = tf.stack(targets[1:])\n",
        "    return features, targets\n",
        "\n",
        "train = train.map(_preprocess_line)\n",
        "train = train.batch(BATCH_SIZE)\n",
        "\n",
        "val = val.map(_preprocess_line)\n",
        "val = val.batch(BATCH_SIZE)\n",
        "\n",
        "test = test.map(_preprocess_line)\n",
        "test = test.batch(BATCH_SIZE)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VlyC9JXFZas",
        "outputId": "37918677-0efd-42d9-8c25-3c8b33e506e8"
      },
      "source": [
        "for feature_batch, label_batch in train.take(1):\n",
        "    print('First 5 features:', list(feature_batch.keys())[:5])\n",
        "    print('A batch of cp_types:', feature_batch['cp_type'].numpy())\n",
        "    print('A batch of cp_times:', feature_batch['cp_time'].numpy())\n",
        "    print('A batch of targets:', label_batch.numpy() ) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 features: ['cp_type', 'cp_time', 'cp_dose', 'g-0', 'g-1']\n",
            "A batch of cp_types: [b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'ctl_vehicle' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp']\n",
            "A batch of cp_times: [b'24' b'72' b'48' b'48' b'72' b'24' b'24' b'48' b'48' b'48' b'72' b'48'\n",
            " b'48' b'48' b'72' b'48' b'48' b'24' b'72' b'48' b'48' b'48' b'72' b'72'\n",
            " b'72' b'48' b'72' b'48' b'48' b'72' b'72' b'48']\n",
            "A batch of targets: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xfo0egbGOkz"
      },
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = feature_ds.map(index)\n",
        "\n",
        "  # Learn the space of possible indices.\n",
        "  encoder.adapt(feature_ds)\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUM4wEAGVSn"
      },
      "source": [
        "encoded_features = []\n",
        "all_inputs = []\n",
        "for header in cols_features[4:]:\n",
        "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "    encoded_numeric_col= tf.linalg.normalize(numeric_col, ord='euclidean', axis=None, name=None)\n",
        "    all_inputs.append(numeric_col)\n",
        "    encoded_features.append(encoded_numeric_col[0]) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnBGLW6uHy9B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMJDPEc_GnXK",
        "outputId": "03031a4d-138c-4395-f9aa-e99f6b03eae7"
      },
      "source": [
        "categorical_cols = ['cp_type', 'cp_dose','cp_time']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n",
        "  print(\"Normalization of \", str(header), \" done !\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalization of  cp_type  done !\n",
            "Normalization of  cp_dose  done !\n",
            "Normalization of  cp_time  done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEGz-249GtoL"
      },
      "source": [
        "all_features = []\n",
        "feature_layer = []\n",
        "all_features = tf.keras.layers.concatenate(encoded_features)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joUZd8e7XWHe"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdp9g7VFXWFZ"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU64MqzZXWBw"
      },
      "source": [
        "from tensorboard.plugins.hparams import api as hp\n",
        "import datetime\n",
        "!rm -rf ./logs/\n",
        "\n",
        "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sxxYb-pXV_L"
      },
      "source": [
        "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([128,256,512]))\n",
        "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([128,256,512]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.5))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','adadelta']))\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','elu']))\n",
        "HP_ACTIVATION_OUTPUT = hp.HParam('activation_output', hp.Discrete(['sigmoid']))\n",
        "\n",
        "METRIC_CATEGORICAL_ACCURACY = \"categorical_accuracy\"\n",
        "METRIC_BINARY_ACCURACY = \"binary_accuracy\"\n",
        "METRIC_CATEGORICAL_CROSSENTROPY = \"categorical_crossentropy\"\n",
        "METRIC_BINARY_CROSSENTROPY = \"binary_crossentropy\"\n",
        "METRIC_MSE = \"mean_squared_error\"\n",
        "\n",
        "metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"]\n",
        "\n",
        "with tf.summary.create_file_writer(logdir).as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS_1, HP_NUM_UNITS_2, HP_DROPOUT, HP_ACTIVATION, HP_ACTIVATION_OUTPUT, HP_OPTIMIZER],\n",
        "    metrics=[ hp.Metric(METRIC_CATEGORICAL_ACCURACY, display_name='Categorical Accuracy'),\n",
        "              hp.Metric(METRIC_BINARY_ACCURACY, display_name='Binary Accuracy'),\n",
        "              hp.Metric(METRIC_CATEGORICAL_CROSSENTROPY, display_name='Categorical Cross Entropy Accuracy'),\n",
        "              hp.Metric(METRIC_BINARY_CROSSENTROPY, display_name='Binary Cross Entropy'),\n",
        "              hp.Metric(METRIC_MSE, display_name='MSE'),\n",
        "    ],\n",
        "  )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpS9Fh2xXV8f"
      },
      "source": [
        "def train_test_model(hparams):\n",
        "\n",
        "  x = tf.keras.layers.BatchNormalization()(all_features)\n",
        "\n",
        "  x =   tf.keras.layers.Dense(hparams[HP_NUM_UNITS_1],activation=hparams[HP_ACTIVATION])(x)\n",
        "  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x =   tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
        "  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  output = tf.keras.layers.Dense(206, activation=hparams[HP_ACTIVATION_OUTPUT])(x)\n",
        "  model = tf.keras.Model(all_inputs,output)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = hparams[HP_OPTIMIZER],\n",
        "      loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"],\n",
        "  )\n",
        "\n",
        "  model.fit(train,\n",
        "            validation_data= val,\n",
        "            epochs=15,\n",
        "            shuffle=True,\n",
        "            verbose =1,\n",
        "            callbacks=[ tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
        "                        hp.KerasCallback(logdir, hparams),  # log hparams\n",
        "                        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=10),\n",
        "    ]) \n",
        "  _, categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = model.evaluate(test)\n",
        "  return categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaXbqQLuafGW"
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_CATEGORICAL_ACCURACY, categorical_accuracy, step=1)\n",
        "    tf.summary.scalar(METRIC_BINARY_ACCURACY, binary_accuracy, step=1)\n",
        "    tf.summary.scalar(METRIC_CATEGORICAL_CROSSENTROPY, categorical_crossentropy, step=1)\n",
        "    tf.summary.scalar(METRIC_BINARY_CROSSENTROPY, binary_crossentropy, step=1)\n",
        "    tf.summary.scalar(METRIC_MSE, mean_squared_error, step=1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUGbrZcyXVwe",
        "outputId": "b13a2cc4-17ff-4673-ced7-867390691988"
      },
      "source": [
        "session_num = 0\n",
        " \n",
        "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
        "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
        "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "          for activation in HP_ACTIVATION.domain.values:\n",
        "            for activation_output in HP_ACTIVATION_OUTPUT.domain.values:\n",
        "              hparams = {\n",
        "                HP_NUM_UNITS_1: num_units_1,\n",
        "                HP_NUM_UNITS_2: num_units_2,\n",
        "                HP_DROPOUT : dropout_rate,\n",
        "                HP_OPTIMIZER: optimizer,\n",
        "                HP_ACTIVATION: activation,\n",
        "                HP_ACTIVATION_OUTPUT: activation_output\n",
        "              }\n",
        "              run_name = \"run-%d\" % session_num\n",
        "              print('--- Starting trial: %s' % run_name)\n",
        "              print({h.name: hparams[h] for h in hparams})\n",
        "              run(logdir + run_name, hparams)\n",
        "              session_num += 1          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-0\n",
            "{'num_units_1': 128, 'num_units_2': 128, 'dropout': 0.2, 'optimizer': 'adadelta', 'activation': 'elu', 'activation_output': 'sigmoid'}\n",
            "Epoch 1/15\n",
            "      1/Unknown - 0s 240us/step - loss: 0.7817 - categorical_accuracy: 0.0000e+00 - binary_accuracy: 0.4995 - categorical_crossentropy: 5.6475 - binary_crossentropy: 0.7817 - mean_squared_error: 0.2846WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "      2/Unknown - 0s 112ms/step - loss: 0.7819 - categorical_accuracy: 0.0000e+00 - binary_accuracy: 0.5010 - categorical_crossentropy: 4.5414 - binary_crossentropy: 0.7819 - mean_squared_error: 0.2849WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0654s vs `on_train_batch_end` time: 0.1584s). Check your callbacks.\n",
            "521/521 [==============================] - 54s 103ms/step - loss: 0.7806 - categorical_accuracy: 0.0034 - binary_accuracy: 0.4999 - categorical_crossentropy: 3.7989 - binary_crossentropy: 0.7806 - mean_squared_error: 0.2845 - val_loss: 0.7458 - val_categorical_accuracy: 0.0042 - val_binary_accuracy: 0.5009 - val_categorical_crossentropy: 3.7506 - val_binary_crossentropy: 0.7458 - val_mean_squared_error: 0.2721\n",
            "Epoch 2/15\n",
            "521/521 [==============================] - 56s 108ms/step - loss: 0.7794 - categorical_accuracy: 0.0032 - binary_accuracy: 0.5008 - categorical_crossentropy: 3.7962 - binary_crossentropy: 0.7794 - mean_squared_error: 0.2840 - val_loss: 0.7498 - val_categorical_accuracy: 0.0045 - val_binary_accuracy: 0.5024 - val_categorical_crossentropy: 3.7527 - val_binary_crossentropy: 0.7498 - val_mean_squared_error: 0.2734\n",
            "Epoch 3/15\n",
            "131/521 [======>.......................] - ETA: 29s - loss: 0.7784 - categorical_accuracy: 0.0043 - binary_accuracy: 0.5005 - categorical_crossentropy: 3.7968 - binary_crossentropy: 0.7784 - mean_squared_error: 0.2837"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwL27BiMXVuF"
      },
      "source": [
        "%tensorboard --logdir logdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eCqLoM7pXV"
      },
      "source": [
        "# Brouillon en **dessous**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYSufAo2XVXR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSwMeAVEUxvr",
        "outputId": "6f0f80a6-1696-4148-aeb7-d45a720acaab"
      },
      "source": [
        "STEPS_PER_EPOCH = train_size//BATCH_SIZE/3-9\n",
        "print(STEPS_PER_EPOCH)\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*1000,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJjK0l_MU3I2"
      },
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhBHdupUUNJ-"
      },
      "source": [
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=[\n",
        "                  tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy'),\n",
        "                  'categorical_accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    train,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_data=val,\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=0)\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkI2tzktVDBL"
      },
      "source": [
        "x_tiny = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "output_tiny = tf.keras.layers.Dense(206)(x_tiny)\n",
        "tiny_model = tf.keras.Model(all_inputs, output_tiny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSxRaxZsYEKG"
      },
      "source": [
        "  size_histories = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwFdNn4nYGEx"
      },
      "source": [
        "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do_F1M0iZXeV"
      },
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccm157uaGukc"
      },
      "source": [
        "x = tf.keras.layers.BatchNormalization()(all_features)\n",
        "\n",
        "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "output = tf.keras.layers.Dense(206, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(all_inputs,output)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vsRgMzrXaxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd641c2b-5de3-4470-947d-74cbeb0d73b4"
      },
      "source": [
        "model.fit(train,validation_data=val,epochs=10)\n",
        "#model.save('MOA_model_1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "521/521 [==============================] - 52s 101ms/step - loss: 0.2355 - categorical_accuracy: 0.0122 - binary_accuracy: 0.9173 - categorical_crossentropy: 3.6129 - binary_crossentropy: 0.2355 - mean_squared_error: 0.0692 - val_loss: 0.0268 - val_categorical_accuracy: 0.0070 - val_binary_accuracy: 0.9966 - val_categorical_crossentropy: 3.3807 - val_binary_crossentropy: 0.0268 - val_mean_squared_error: 0.0034\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 49s 94ms/step - loss: 0.0240 - categorical_accuracy: 0.0268 - binary_accuracy: 0.9966 - categorical_crossentropy: 3.3447 - binary_crossentropy: 0.0240 - mean_squared_error: 0.0035 - val_loss: 0.0202 - val_categorical_accuracy: 0.0291 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.2113 - val_binary_crossentropy: 0.0202 - val_mean_squared_error: 0.0032\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 50s 97ms/step - loss: 0.0211 - categorical_accuracy: 0.0328 - binary_accuracy: 0.9965 - categorical_crossentropy: 3.2203 - binary_crossentropy: 0.0211 - mean_squared_error: 0.0034 - val_loss: 0.0196 - val_categorical_accuracy: 0.0266 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.1381 - val_binary_crossentropy: 0.0196 - val_mean_squared_error: 0.0032\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 51s 97ms/step - loss: 0.0205 - categorical_accuracy: 0.0331 - binary_accuracy: 0.9966 - categorical_crossentropy: 3.1713 - binary_crossentropy: 0.0205 - mean_squared_error: 0.0033 - val_loss: 0.0194 - val_categorical_accuracy: 0.0266 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.1074 - val_binary_crossentropy: 0.0194 - val_mean_squared_error: 0.0032\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 49s 95ms/step - loss: 0.0202 - categorical_accuracy: 0.0352 - binary_accuracy: 0.9966 - categorical_crossentropy: 3.1436 - binary_crossentropy: 0.0202 - mean_squared_error: 0.0033 - val_loss: 0.0192 - val_categorical_accuracy: 0.0260 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.0870 - val_binary_crossentropy: 0.0192 - val_mean_squared_error: 0.0031\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 49s 95ms/step - loss: 0.0199 - categorical_accuracy: 0.0350 - binary_accuracy: 0.9966 - categorical_crossentropy: 3.1215 - binary_crossentropy: 0.0199 - mean_squared_error: 0.0033 - val_loss: 0.0190 - val_categorical_accuracy: 0.0288 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.0614 - val_binary_crossentropy: 0.0190 - val_mean_squared_error: 0.0031\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 52s 99ms/step - loss: 0.0196 - categorical_accuracy: 0.0370 - binary_accuracy: 0.9967 - categorical_crossentropy: 3.1002 - binary_crossentropy: 0.0196 - mean_squared_error: 0.0032 - val_loss: 0.0188 - val_categorical_accuracy: 0.0381 - val_binary_accuracy: 0.9968 - val_categorical_crossentropy: 3.0431 - val_binary_crossentropy: 0.0188 - val_mean_squared_error: 0.0031\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 50s 96ms/step - loss: 0.0194 - categorical_accuracy: 0.0414 - binary_accuracy: 0.9967 - categorical_crossentropy: 3.0774 - binary_crossentropy: 0.0194 - mean_squared_error: 0.0032 - val_loss: 0.0187 - val_categorical_accuracy: 0.0442 - val_binary_accuracy: 0.9969 - val_categorical_crossentropy: 3.0210 - val_binary_crossentropy: 0.0187 - val_mean_squared_error: 0.0031\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 51s 97ms/step - loss: 0.0192 - categorical_accuracy: 0.0479 - binary_accuracy: 0.9967 - categorical_crossentropy: 3.0520 - binary_crossentropy: 0.0192 - mean_squared_error: 0.0032 - val_loss: 0.0185 - val_categorical_accuracy: 0.0546 - val_binary_accuracy: 0.9969 - val_categorical_crossentropy: 2.9909 - val_binary_crossentropy: 0.0185 - val_mean_squared_error: 0.0031\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 52s 99ms/step - loss: 0.0190 - categorical_accuracy: 0.0544 - binary_accuracy: 0.9968 - categorical_crossentropy: 3.0180 - binary_crossentropy: 0.0190 - mean_squared_error: 0.0032 - val_loss: 0.0182 - val_categorical_accuracy: 0.0605 - val_binary_accuracy: 0.9969 - val_categorical_crossentropy: 2.9399 - val_binary_crossentropy: 0.0182 - val_mean_squared_error: 0.0030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffad41edb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrO-R1ljXi6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a6970b-1080-4325-8511-6f07c3ceabe6"
      },
      "source": [
        "model.evaluate(test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112/112 [==============================] - 9s 77ms/step - loss: 0.0182 - categorical_accuracy: 0.0605 - binary_accuracy: 0.9969 - categorical_crossentropy: 2.9399 - binary_crossentropy: 0.0182 - mean_squared_error: 0.0030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.018200933933258057,\n",
              " 0.060470324009656906,\n",
              " 0.9968891143798828,\n",
              " 2.9398598670959473,\n",
              " 0.018200933933258057,\n",
              " 0.003036072012037039]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX_CqLUwGv2T"
      },
      "source": [
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
        "#model.save(\"drive/MyDrive/lish-moa\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}