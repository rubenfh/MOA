{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rubenfh/MOA/blob/dev%2Falex/MOA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TepgBQy2FG07",
    "outputId": "07c8c912-41f3-4f59-8f25-b53798f3bc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# ML library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow import feature_column\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow_addons as tfa\n",
    "#Tensor Flow doc\n",
    "!pip3 install -q git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data manipulation library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "#Other library\n",
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import uuid \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M9H-jbV4Y7KZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5M_f_jW9FN_z",
    "outputId": "0fcc2dcc-32c0-4f8e-a9a2-97dcca44fca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    TRAIN_FEATURES_PATH =  \"drive/MyDrive/lish-moa/train_features.csv\"\n",
    "    TRAIN_LABELS_PATH   =  \"drive/MyDrive/lish-moa/train_targets_scored.csv\"\n",
    "    TEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/test_features.csv\"\n",
    "    BEST_FEATURES_PATH  =  \"drive/MyDrive/lish-moa/best_features.csv\"\n",
    "else:\n",
    "    TRAIN_FEATURES_PATH =  \"lish-moa/train_features.csv\"\n",
    "    TRAIN_LABELS_PATH   =  \"lish-moa/train_targets_scored.csv\"\n",
    "    TEST_FEATURES_PATH  =  \"lish-moa/test_features.csv\"\n",
    "    BEST_FEATURES_PATH  =  \"lish-moa/best_features.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d4DSQ1vFQcI",
    "outputId": "fa84e96d-8391-41eb-abfa-94d40d0c9eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 876\n",
      "Number of targets: 207\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv(TRAIN_FEATURES_PATH, nrows=10)\n",
    "targets = pd.read_csv(TRAIN_LABELS_PATH, nrows=10)\n",
    "#best_features = pd.read_csv(BEST_FEATURES_PATH)\n",
    "cols_features = features.columns\n",
    "cols_targets = targets.columns\n",
    "\n",
    "num_features = len(cols_features) \n",
    "num_targets = len(cols_targets)\n",
    "print(\"Number of features:\" , num_features)\n",
    "print(\"Number of targets:\" , num_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zV2rhk9VFS0-"
   },
   "outputs": [],
   "source": [
    "features_types = [str(), str(), str(), str()] + [float()]*(num_features-4)\n",
    "targets_types = [str()] + [float()]*(num_targets-1)\n",
    "\n",
    "features = tf.data.experimental.CsvDataset(TRAIN_FEATURES_PATH,\n",
    "                                           record_defaults=features_types,\n",
    "                                           #select_cols\n",
    "                                           header=True)\n",
    "\n",
    "targets = tf.data.experimental.CsvDataset(TRAIN_LABELS_PATH,\n",
    "                                          record_defaults=targets_types,\n",
    "                                          header=True)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((features, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcQFQfg_FTwC",
    "outputId": "4ba7136a-9ea0-4948-d42a-b1c0522a2b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 23814\n",
      "Train dataset size: 16669\n",
      "Val dataset size: 3572\n",
      "Test dataset size: 3572\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train and val\n",
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "train_size = int(0.7*dataset_size)\n",
    "val_size = int(0.15*dataset_size)\n",
    "test_size = int(0.15*dataset_size)\n",
    "\n",
    "train = dataset.take(train_size)\n",
    "val = dataset.skip(train_size)\n",
    "val = dataset.take(val_size)\n",
    "test = dataset.skip(train_size + val_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"Full dataset size:\", dataset_size)\n",
    "print(\"Train dataset size:\", train_size)\n",
    "print(\"Val dataset size:\", val_size)\n",
    "print(\"Test dataset size:\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y0LkSqsSFXm7"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def _preprocess_line(features, targets):\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(cols_features, features))\n",
    "    features.pop('sig_id')\n",
    "    targets = tf.stack(targets[1:])\n",
    "    return features, targets\n",
    "\n",
    "train = train.map(_preprocess_line)\n",
    "train = train.batch(BATCH_SIZE)\n",
    "\n",
    "val = val.map(_preprocess_line)\n",
    "val = val.batch(BATCH_SIZE)\n",
    "\n",
    "test = test.map(_preprocess_line)\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VlyC9JXFZas",
    "outputId": "5162b9ee-6722-4f0b-a16d-f5511eda8784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 features: ['cp_type', 'cp_time', 'cp_dose', 'g-0', 'g-1']\n",
      "A batch of cp_types: [b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
      " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
      " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
      " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'ctl_vehicle' b'trt_cp'\n",
      " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp']\n",
      "A batch of cp_times: [b'24' b'72' b'48' b'48' b'72' b'24' b'24' b'48' b'48' b'48' b'72' b'48'\n",
      " b'48' b'48' b'72' b'48' b'48' b'24' b'72' b'48' b'48' b'48' b'72' b'72'\n",
      " b'72' b'48' b'72' b'48' b'48' b'72' b'72' b'48']\n",
      "A batch of targets: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train.take(1):\n",
    "    print('First 5 features:', list(feature_batch.keys())[:5])\n",
    "    print('A batch of cp_types:', feature_batch['cp_type'].numpy())\n",
    "    print('A batch of cp_times:', feature_batch['cp_time'].numpy())\n",
    "    print('A batch of targets:', label_batch.numpy() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6xfo0egbGOkz"
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = feature_ds.map(index)\n",
    "\n",
    "  # Learn the space of possible indices.\n",
    "  encoder.adapt(feature_ds)\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lDUM4wEAGVSn"
   },
   "outputs": [],
   "source": [
    "encoded_features = []\n",
    "all_inputs = []\n",
    "for header in cols_features[4:]:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    encoded_numeric_col= tf.linalg.normalize(numeric_col, ord='euclidean', axis=None, name=None)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMJDPEc_GnXK",
    "outputId": "1e25dfb6-9837-4237-90a5-a6990cd15770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization of  cp_type  done !\n",
      "Normalization of  cp_dose  done !\n",
      "Normalization of  cp_time  done !\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['cp_type', 'cp_dose','cp_time']\n",
    "for header in categorical_cols:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train, dtype='string',\n",
    "                                               max_tokens=5)\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n",
    "  print(\"Normalization of \", str(header), \" done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zEGz-249GtoL"
   },
   "outputs": [],
   "source": [
    "all_features = []\n",
    "feature_layer = []\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "joUZd8e7XWHe"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xdp9g7VFXWFZ"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nU64MqzZXWBw"
   },
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime\n",
    "!rm -rf ./logs/\n",
    "\n",
    "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5sxxYb-pXV_L"
   },
   "outputs": [],
   "source": [
    "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([256,512]))\n",
    "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([256,512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','adadelta']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','elu']))\n",
    "HP_ACTIVATION_OUTPUT = hp.HParam('activation_output', hp.Discrete(['sigmoid']))\n",
    "\n",
    "METRIC_CATEGORICAL_ACCURACY = \"categorical_accuracy\"\n",
    "METRIC_BINARY_ACCURACY = \"binary_accuracy\"\n",
    "METRIC_CATEGORICAL_CROSSENTROPY = \"categorical_crossentropy\"\n",
    "METRIC_BINARY_CROSSENTROPY = \"binary_crossentropy\"\n",
    "METRIC_MSE = \"mean_squared_error\"\n",
    "\n",
    "metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"]\n",
    "\n",
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS_1, HP_NUM_UNITS_2, HP_DROPOUT, HP_ACTIVATION, HP_ACTIVATION_OUTPUT, HP_OPTIMIZER],\n",
    "    metrics=[ hp.Metric(METRIC_CATEGORICAL_ACCURACY, display_name='Categorical Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_ACCURACY, display_name='Binary Accuracy'),\n",
    "              hp.Metric(METRIC_CATEGORICAL_CROSSENTROPY, display_name='Categorical Cross Entropy Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_CROSSENTROPY, display_name='Binary Cross Entropy'),\n",
    "              hp.Metric(METRIC_MSE, display_name='MSE'),\n",
    "    ],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CpS9Fh2xXV8f"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "\n",
    "  x = tf.keras.layers.BatchNormalization()(all_features)\n",
    "\n",
    "  x =   tf.keras.layers.Dense(hparams[HP_NUM_UNITS_1],activation=hparams[HP_ACTIVATION])(x)\n",
    "  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x =   tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
    "  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  output = tf.keras.layers.Dense(206, activation=hparams[HP_ACTIVATION_OUTPUT])(x)\n",
    "  model = tf.keras.Model(all_inputs,output)\n",
    "\n",
    "  model.compile(\n",
    "      optimizer = hparams[HP_OPTIMIZER],\n",
    "      loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"],\n",
    "  )\n",
    "\n",
    "  model.fit(train,\n",
    "            validation_data= val,\n",
    "            epochs=10,\n",
    "            shuffle=True,\n",
    "            verbose =1,\n",
    "            callbacks=[ tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
    "                        hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=10),\n",
    "    ]) \n",
    "  _, categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = model.evaluate(test)\n",
    "  return categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CaXbqQLuafGW"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_CATEGORICAL_ACCURACY, categorical_accuracy, step=1)\n",
    "    tf.summary.scalar(METRIC_BINARY_ACCURACY, binary_accuracy, step=1)\n",
    "    tf.summary.scalar(METRIC_CATEGORICAL_CROSSENTROPY, categorical_crossentropy, step=1)\n",
    "    tf.summary.scalar(METRIC_BINARY_CROSSENTROPY, binary_crossentropy, step=1)\n",
    "    tf.summary.scalar(METRIC_MSE, mean_squared_error, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUGbrZcyXVwe",
    "outputId": "5aaf2181-1e1a-4dab-eb1b-510fa0d8f9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units_1': 256, 'num_units_2': 256, 'dropout': 0.2, 'optimizer': 'adadelta', 'activation': 'elu', 'activation_output': 'sigmoid'}\n",
      "Epoch 1/10\n",
      "      2/Unknown - 2s 1s/step - loss: 0.8121 - categorical_accuracy: 0.0156 - binary_accuracy: 0.5011 - categorical_crossentropy: 4.3924 - binary_crossentropy: 0.8121 - mean_squared_error: 0.2942       WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0765s vs `on_train_batch_end` time: 2.0852s). Check your callbacks.\n",
      "    294/Unknown - 23s 78ms/step - loss: 0.8155 - categorical_accuracy: 0.0055 - binary_accuracy: 0.4992 - categorical_crossentropy: 3.8393 - binary_crossentropy: 0.8155 - mean_squared_error: 0.2953"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    " \n",
    "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
    "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "          for activation in HP_ACTIVATION.domain.values:\n",
    "            for activation_output in HP_ACTIVATION_OUTPUT.domain.values:\n",
    "              hparams = {\n",
    "                HP_NUM_UNITS_1: num_units_1,\n",
    "                HP_NUM_UNITS_2: num_units_2,\n",
    "                HP_DROPOUT : dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_ACTIVATION_OUTPUT: activation_output\n",
    "              }\n",
    "              run_name = \"run-%d\" % session_num\n",
    "              print('--- Starting trial: %s' % run_name)\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              run(logdir + run_name, hparams)\n",
    "              session_num += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwL27BiMXVuF"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5eCqLoM7pXV"
   },
   "source": [
    "# Brouillon en **dessous**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYSufAo2XVXR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSwMeAVEUxvr"
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = train_size//BATCH_SIZE/3-9\n",
    "print(STEPS_PER_EPOCH)\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJjK0l_MU3I2"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhBHdupUUNJ-"
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=10):\n",
    "  if optimizer is None:\n",
    "    optimizer = get_optimizer()\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy'),\n",
    "                  'categorical_accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  history = model.fit(\n",
    "    train,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=val,\n",
    "    callbacks=get_callbacks(name),\n",
    "    verbose=0)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkI2tzktVDBL"
   },
   "outputs": [],
   "source": [
    "x_tiny = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "output_tiny = tf.keras.layers.Dense(206)(x_tiny)\n",
    "tiny_model = tf.keras.Model(all_inputs, output_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSxRaxZsYEKG"
   },
   "outputs": [],
   "source": [
    "  size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFdNn4nYGEx"
   },
   "outputs": [],
   "source": [
    "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do_F1M0iZXeV"
   },
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccm157uaGukc"
   },
   "outputs": [],
   "source": [
    "x = tf.keras.layers.BatchNormalization()(all_features)\n",
    "\n",
    "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(206, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(all_inputs,output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vsRgMzrXaxT"
   },
   "outputs": [],
   "source": [
    "model.fit(train,validation_data=val,epochs=10)\n",
    "#model.save('MOA_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrO-R1ljXi6t"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX_CqLUwGv2T"
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "#model.save(\"drive/MyDrive/lish-moa\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMXRyBtUnsuzYWYEYAcpkiQ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MOA",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
